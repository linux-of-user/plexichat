{
  "name": "ai_providers",
  "version": "1.0.0",
  "description": "AI providers with BitNet 1-bit LLM and Llama support",
  "author": "PlexiChat Team",
  "type": "ai_provider",
  "enabled": true,
  "priority": 1,
  "dependencies": [],
  "permissions": [
    "ai_access",
    "file_system_access",
    "network_access",
    "database_access",
    "webui_access"
  ],
  "capabilities": [
    "bitnet_1bit_llm",
    "llama_cpp",
    "hf_integration",
    "local_inference",
    "kernel_optimization",
    "streaming_inference"
  ],
  "entry_point": "main.py",
  "config_schema": {
    "bitnet": {
      "enabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable BitNet 1-bit LLM support"
      },
      "model_path": {
        "type": "string",
        "default": "data/bitnet_models",
        "description": "Path to BitNet models"
      },
      "kernel_optimization": {
        "type": "boolean",
        "default": true,
        "description": "Enable kernel optimization"
      }
    },
    "llama": {
      "enabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable Llama.cpp support"
      },
      "model_path": {
        "type": "string",
        "default": "data/llama_models",
        "description": "Path to Llama models"
      },
      "n_ctx": {
        "type": "integer",
        "default": 2048,
        "description": "Context window size"
      }
    },
    "hf": {
      "enabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable HuggingFace integration"
      },
      "cache_dir": {
        "type": "string",
        "default": "data/hf_cache",
        "description": "HuggingFace cache directory"
      }
    }
  },
  "webui": {
    "enabled": true,
    "routes": [
      {
        "path": "/ai-providers",
        "component": "AIPanel",
        "title": "AI Providers",
        "icon": "brain"
      }
    ]
  },
  "self_tests": [
    "test_bitnet",
    "test_llama",
    "test_hf",
    "test_inference"
  ],
  "requirements": [
    "numpy>=1.21.0",
    "torch>=1.9.0",
    "transformers>=4.20.0",
    "llama-cpp-python>=0.2.0"
  ]
}
