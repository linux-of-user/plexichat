{
  "name": "ai_providers",
  "version": "1.0.0",
  "description": "Advanced AI providers plugin with BitNet 1-bit LLM and Llama support",
  "author": "PlexiChat Team",
  "type": "ai_provider",
  "enabled": true,
  "priority": 1,
  "dependencies": [],
  "permissions": [
    "ai_access",
    "file_system_access",
    "network_access",
    "database_access",
    "webui_access"
  ],
  "capabilities": [
    "bitnet_1bit_llm",
    "llama_cpp",
    "huggingface_integration",
    "local_inference",
    "kernel_optimization",
    "streaming_inference",
    "model_management"
  ],
  "entry_point": "main.py",
  "config_schema": {
    "bitnet": {
      "enabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable BitNet 1-bit LLM support"
      },
      "model_path": {
        "type": "string",
        "default": "data/bitnet_models",
        "description": "Path to BitNet models"
      },
      "kernel_optimization": {
        "type": "boolean",
        "default": true,
        "description": "Enable kernel optimization"
      },
      "use_gpu": {
        "type": "boolean",
        "default": true,
        "description": "Use GPU acceleration if available"
      }
    },
    "llama": {
      "enabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable Llama.cpp support"
      },
      "model_path": {
        "type": "string",
        "default": "data/llama_models",
        "description": "Path to Llama models"
      },
      "n_ctx": {
        "type": "integer",
        "default": 2048,
        "description": "Context window size"
      },
      "n_gpu_layers": {
        "type": "integer",
        "default": 0,
        "description": "Number of GPU layers"
      }
    },
    "huggingface": {
      "enabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable HuggingFace integration"
      },
      "cache_dir": {
        "type": "string",
        "default": "data/hf_cache",
        "description": "HuggingFace cache directory"
      },
      "use_auth_token": {
        "type": "boolean",
        "default": false,
        "description": "Use HuggingFace auth token"
      }
    }
  },
  "webui": {
    "enabled": true,
    "routes": [
      {
        "path": "/ai-providers",
        "component": "AIProvidersPanel",
        "title": "AI Providers",
        "icon": "brain"
      },
      {
        "path": "/ai-providers/models",
        "component": "ModelManager",
        "title": "Model Management",
        "icon": "database"
      },
      {
        "path": "/ai-providers/benchmark",
        "component": "BenchmarkPanel",
        "title": "Performance Benchmark",
        "icon": "chart-line"
      }
    ]
  },
  "self_tests": [
    "test_bitnet_provider",
    "test_llama_provider",
    "test_hf_integration",
    "test_model_loading",
    "test_inference",
    "test_streaming",
    "test_kernel_optimization"
  ],
  "requirements": [
    "numpy>=1.21.0",
    "torch>=1.9.0",
    "transformers>=4.20.0",
    "llama-cpp-python>=0.2.0",
    "huggingface-hub>=0.15.0"
  ],
  "system_requirements": {
    "min_memory_gb": 4,
    "min_disk_gb": 10,
    "python_version": ">=3.8"
  }
}
