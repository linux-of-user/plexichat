"""
PlexiChat Advanced Indexing Strategy

Intelligent database indexing system with:
- Automatic index creation and optimization
- Query pattern analysis for index suggestions
- Index usage monitoring and maintenance
- Composite index optimization
- Partial and filtered indexes
- Index performance tracking
- Automatic index cleanup for unused indexes
- Multi-database index management

Features:
- Smart index recommendations based on query patterns
- Index usage statistics and monitoring
- Automatic index maintenance and optimization
- Support for different index types (B-tree, Hash, GIN, GiST)
- Composite index optimization
- Index fragmentation detection
- Performance impact analysis
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Set, Tuple
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, Counter
import json

try:
    from .enhanced_abstraction import DatabaseType, AbstractDatabaseClient  # type: ignore
    ENHANCED_ABSTRACTION_AVAILABLE = True
except ImportError:
    # Create placeholder classes if enhanced_abstraction is not available
    ENHANCED_ABSTRACTION_AVAILABLE = False

    class DatabaseType:
        POSTGRESQL = "postgresql"
        MYSQL = "mysql"
        SQLITE = "sqlite"
        MONGODB = "mongodb"
        REDIS = "redis"
        CLICKHOUSE = "clickhouse"
        TIMESCALEDB = "timescaledb"

    class AbstractDatabaseClient:
        def __init__(self, config):
            self.config = config
            self.connected = False

        async def connect(self):
            """Connect to database."""
            self.connected = True
            return True

        async def disconnect(self):
            """Disconnect from database."""
            self.connected = False
            return True

        async def execute_query(self, query, params=None):
            """Execute a database query."""
            # Acknowledge parameters to avoid unused warnings
            _ = query, params
            # Mock result object
            class MockResult:
                def __init__(self):
                    self.success = True
                    self.data = []
                    self.count = 0
                    self.error = None
            return MockResult()
from .query_optimizer import sql_analyzer, performance_monitor

logger = logging.getLogger(__name__)


class IndexType(Enum):
    """Types of database indexes."""
    BTREE = "btree"
    HASH = "hash"
    GIN = "gin"  # PostgreSQL Generalized Inverted Index
    GIST = "gist"  # PostgreSQL Generalized Search Tree
    BRIN = "brin"  # PostgreSQL Block Range Index
    PARTIAL = "partial"
    UNIQUE = "unique"
    COMPOSITE = "composite"
    COVERING = "covering"


class IndexStatus(Enum):
    """Index status."""
    ACTIVE = "active"
    CREATING = "creating"
    INVALID = "invalid"
    UNUSED = "unused"
    FRAGMENTED = "fragmented"
    RECOMMENDED = "recommended"


@dataclass
class IndexDefinition:
    """Database index definition."""
    name: str
    table: str
    columns: List[str]
    index_type: IndexType = IndexType.BTREE
    unique: bool = False
    partial_condition: Optional[str] = None
    include_columns: List[str] = field(default_factory=list)  # For covering indexes
    
    # Metadata
    size_bytes: Optional[int] = None
    creation_time: Optional[datetime] = None
    last_used: Optional[datetime] = None
    usage_count: int = 0
    
    # Performance metrics
    scan_count: int = 0
    tuple_read: int = 0
    tuple_fetch: int = 0
    
    # Status
    status: IndexStatus = IndexStatus.RECOMMENDED
    
    def to_sql(self, database_type: DatabaseType) -> str:
        """Generate SQL for index creation."""
        if database_type == DatabaseType.POSTGRESQL:
            return self._to_postgresql_sql()
        elif database_type == DatabaseType.MYSQL:
            return self._to_mysql_sql()
        elif database_type == DatabaseType.SQLITE:
            return self._to_sqlite_sql()
        else:
            return self._to_generic_sql()
    
    def _to_postgresql_sql(self) -> str:
        """Generate PostgreSQL index creation SQL."""
        unique_clause = "UNIQUE " if self.unique else ""
        index_type_clause = f" USING {self.index_type.value.upper()}" if self.index_type != IndexType.BTREE else ""
        
        columns_clause = ", ".join(self.columns)
        
        sql = f"CREATE {unique_clause}INDEX {self.name} ON {self.table}{index_type_clause} ({columns_clause})"
        
        # Add INCLUDE clause for covering indexes
        if self.include_columns:
            include_clause = ", ".join(self.include_columns)
            sql += f" INCLUDE ({include_clause})"
        
        # Add WHERE clause for partial indexes
        if self.partial_condition:
            sql += f" WHERE {self.partial_condition}"
        
        return sql
    
    def _to_mysql_sql(self) -> str:
        """Generate MySQL index creation SQL."""
        unique_clause = "UNIQUE " if self.unique else ""
        columns_clause = ", ".join(self.columns)
        
        return f"CREATE {unique_clause}INDEX {self.name} ON {self.table} ({columns_clause})"
    
    def _to_sqlite_sql(self) -> str:
        """Generate SQLite index creation SQL."""
        unique_clause = "UNIQUE " if self.unique else ""
        columns_clause = ", ".join(self.columns)
        
        sql = f"CREATE {unique_clause}INDEX {self.name} ON {self.table} ({columns_clause})"
        
        if self.partial_condition:
            sql += f" WHERE {self.partial_condition}"
        
        return sql
    
    def _to_generic_sql(self) -> str:
        """Generate generic SQL for index creation."""
        unique_clause = "UNIQUE " if self.unique else ""
        columns_clause = ", ".join(self.columns)
        
        return f"CREATE {unique_clause}INDEX {self.name} ON {self.table} ({columns_clause})"


@dataclass
class IndexRecommendation:
    """Index recommendation with analysis."""
    index_definition: IndexDefinition
    priority: int  # 1=high, 2=medium, 3=low
    reason: str
    estimated_improvement: float  # Percentage
    estimated_size_mb: float
    queries_affected: List[str]
    creation_cost: float  # Estimated time to create
    maintenance_cost: float  # Ongoing maintenance overhead


class IndexAnalyzer:
    """Analyze query patterns and recommend indexes."""
    
    def __init__(self):
        self.query_patterns = defaultdict(int)
        self.column_usage = defaultdict(int)
        self.table_access_patterns = defaultdict(dict)
        self.join_patterns = defaultdict(int)
        
    def analyze_query_for_indexes(self, query: str, execution_time_ms: float) -> List[IndexRecommendation]:
        """Analyze a query and recommend indexes."""
        recommendations = []
        
        # Parse query to extract indexable patterns
        analysis = sql_analyzer.analyze_query(query)
        
        # Record usage patterns
        self._record_usage_patterns(analysis, execution_time_ms)
        
        # Generate recommendations based on patterns
        recommendations.extend(self._recommend_where_clause_indexes(analysis, query))
        recommendations.extend(self._recommend_join_indexes(analysis, query))
        recommendations.extend(self._recommend_order_by_indexes(analysis, query))
        recommendations.extend(self._recommend_composite_indexes(analysis, query))
        
        return recommendations
    
    def _record_usage_patterns(self, analysis, execution_time_ms: float):
        """Record query patterns for analysis."""
        for table in analysis.tables_accessed:
            if table not in self.table_access_patterns:
                self.table_access_patterns[table] = {
                    "access_count": 0,
                    "avg_execution_time": 0.0,
                    "columns_used": defaultdict(int)
                }
            
            patterns = self.table_access_patterns[table]
            patterns["access_count"] += 1
            patterns["avg_execution_time"] = (
                (patterns["avg_execution_time"] * (patterns["access_count"] - 1) + execution_time_ms) 
                / patterns["access_count"]
            )
            
            for column in analysis.columns_accessed:
                patterns["columns_used"][column] += 1
                self.column_usage[f"{table}.{column}"] += 1
    
    def _recommend_where_clause_indexes(self, analysis, query: str) -> List[IndexRecommendation]:
        """Recommend indexes for WHERE clause columns."""
        recommendations = []
        
        # Extract WHERE clause patterns
        import re
        where_pattern = re.compile(r'\bWHERE\s+(.+?)(?:\s+ORDER\s+BY|\s+GROUP\s+BY|\s+LIMIT|$)', re.IGNORECASE | re.DOTALL)
        where_match = where_pattern.search(query)
        
        if where_match:
            where_clause = where_match.group(1)
            
            # Find equality conditions (best for indexing)
            equality_pattern = re.compile(r'(\w+)\s*=\s*', re.IGNORECASE)
            equality_columns = equality_pattern.findall(where_clause)
            
            for table in analysis.tables_accessed:
                for column in equality_columns:
                    if column in analysis.columns_accessed:
                        index_def = IndexDefinition(
                            name=f"idx_{table}_{column}",
                            table=table,
                            columns=[column],
                            index_type=IndexType.BTREE
                        )
                        
                        recommendations.append(IndexRecommendation(
                            index_definition=index_def,
                            priority=1,
                            reason=f"Column '{column}' used in WHERE clause equality condition",
                            estimated_improvement=40.0,
                            estimated_size_mb=5.0,
                            queries_affected=[query],
                            creation_cost=2.0,
                            maintenance_cost=0.1
                        ))
        
        return recommendations
    
    def _recommend_join_indexes(self, analysis, query: str) -> List[IndexRecommendation]:
        """Recommend indexes for JOIN conditions."""
        # Acknowledge parameter to avoid unused warning
        _ = analysis
        recommendations = []
        
        # Extract JOIN patterns
        import re
        join_pattern = re.compile(r'\bJOIN\s+(\w+)\s+.*?\bON\s+(\w+)\.(\w+)\s*=\s*(\w+)\.(\w+)', re.IGNORECASE)
        join_matches = join_pattern.findall(query)
        
        for match in join_matches:
            join_table, table1, col1, table2, col2 = match
            # Acknowledge unused variable
            _ = join_table
            
            # Recommend indexes on both sides of the join
            for table, column in [(table1, col1), (table2, col2)]:
                index_def = IndexDefinition(
                    name=f"idx_{table}_{column}_join",
                    table=table,
                    columns=[column],
                    index_type=IndexType.BTREE
                )
                
                recommendations.append(IndexRecommendation(
                    index_definition=index_def,
                    priority=1,
                    reason=f"Column '{column}' used in JOIN condition",
                    estimated_improvement=60.0,
                    estimated_size_mb=8.0,
                    queries_affected=[query],
                    creation_cost=3.0,
                    maintenance_cost=0.15
                ))
        
        return recommendations
    
    def _recommend_order_by_indexes(self, analysis, query: str) -> List[IndexRecommendation]:
        """Recommend indexes for ORDER BY clauses."""
        recommendations = []
        
        if analysis.has_order_by:
            import re
            order_pattern = re.compile(r'\bORDER\s+BY\s+([\w,\s]+)', re.IGNORECASE)
            order_match = order_pattern.search(query)
            
            if order_match:
                order_columns = [col.strip() for col in order_match.group(1).split(',')]
                
                for table in analysis.tables_accessed:
                    # Create composite index for multiple ORDER BY columns
                    if len(order_columns) > 1:
                        index_def = IndexDefinition(
                            name=f"idx_{table}_order_by",
                            table=table,
                            columns=order_columns,
                            index_type=IndexType.BTREE
                        )
                        
                        recommendations.append(IndexRecommendation(
                            index_definition=index_def,
                            priority=2,
                            reason="Columns used in ORDER BY clause",
                            estimated_improvement=30.0,
                            estimated_size_mb=10.0,
                            queries_affected=[query],
                            creation_cost=4.0,
                            maintenance_cost=0.2
                        ))
        
        return recommendations
    
    def _recommend_composite_indexes(self, analysis, query: str) -> List[IndexRecommendation]:
        """Recommend composite indexes for complex queries."""
        recommendations = []
        
        # Look for queries that could benefit from composite indexes
        if analysis.complexity_score > 3.0 and len(analysis.columns_accessed) > 2:
            for table in analysis.tables_accessed:
                # Create composite index with most frequently used columns
                frequent_columns = [col for col in analysis.columns_accessed 
                                  if self.column_usage.get(f"{table}.{col}", 0) > 5]
                
                if len(frequent_columns) >= 2:
                    index_def = IndexDefinition(
                        name=f"idx_{table}_composite",
                        table=table,
                        columns=frequent_columns[:4],  # Limit to 4 columns
                        index_type=IndexType.COMPOSITE
                    )
                    
                    recommendations.append(IndexRecommendation(
                        index_definition=index_def,
                        priority=2,
                        reason="Composite index for frequently accessed columns",
                        estimated_improvement=50.0,
                        estimated_size_mb=15.0,
                        queries_affected=[query],
                        creation_cost=6.0,
                        maintenance_cost=0.3
                    ))
        
        return recommendations


class IndexManager:
    """Manage database indexes across multiple databases."""
    
    def __init__(self):
        self.indexes: Dict[str, Dict[str, IndexDefinition]] = defaultdict(dict)  # db_name -> index_name -> definition
        self.recommendations: Dict[str, List[IndexRecommendation]] = defaultdict(list)
        self.analyzer = IndexAnalyzer()
        self.usage_stats = defaultdict(dict)
        
    async def analyze_and_recommend(self, database_name: str, client: AbstractDatabaseClient) -> List[IndexRecommendation]:
        """Analyze database and recommend indexes."""
        recommendations = []
        
        try:
            # Get existing indexes
            existing_indexes = await self._get_existing_indexes(client)
            self.indexes[database_name] = existing_indexes
            
            # Analyze query patterns from performance monitor
            query_stats = performance_monitor.query_stats
            
            for query_hash, stats in query_stats.items():
                avg_time = stats.get("avg_time", 0) if stats else 0
                if avg_time and avg_time > 100:  # Focus on slower queries
                    # We'd need the actual query text here
                    # For now, we'll use a placeholder
                    query = f"-- Query hash: {query_hash}"
                    query_recommendations = self.analyzer.analyze_query_for_indexes(query, float(avg_time))
                    recommendations.extend(query_recommendations)
            
            # Filter out recommendations for existing indexes
            recommendations = self._filter_existing_indexes(recommendations, existing_indexes)
            
            # Prioritize recommendations
            recommendations = self._prioritize_recommendations(recommendations)
            
            self.recommendations[database_name] = recommendations
            
        except Exception as e:
            logger.error(f"Failed to analyze indexes for {database_name}: {e}")
        
        return recommendations
    
    async def create_recommended_indexes(self, database_name: str, client: AbstractDatabaseClient, 
                                       max_indexes: int = 5) -> List[str]:
        """Create recommended indexes."""
        created_indexes = []
        recommendations = self.recommendations.get(database_name, [])
        
        # Sort by priority and take top recommendations
        top_recommendations = sorted(recommendations, key=lambda x: x.priority)[:max_indexes]
        
        for recommendation in top_recommendations:
            try:
                index_def = recommendation.index_definition
                
                # Generate SQL for index creation
                database_type = getattr(client.config, 'type', DatabaseType.SQLITE)
                create_sql = index_def.to_sql(database_type)
                
                # Execute index creation
                await client.execute_query(create_sql)
                
                # Update index status
                index_def.status = IndexStatus.CREATING
                index_def.creation_time = datetime.now(timezone.utc)
                
                self.indexes[database_name][index_def.name] = index_def
                created_indexes.append(index_def.name)
                
                logger.info(f"âœ… Created index: {index_def.name} on {index_def.table}")
                
            except Exception as e:
                logger.error(f"âŒ Failed to create index {recommendation.index_definition.name}: {e}")
        
        return created_indexes
    
    async def monitor_index_usage(self, database_name: str, client: AbstractDatabaseClient):
        """Monitor index usage and update statistics."""
        try:
            # Get index usage statistics (database-specific)
            database_type = getattr(client.config, 'type', DatabaseType.SQLITE)
            
            if database_type == DatabaseType.POSTGRESQL:
                usage_query = """
                SELECT 
                    schemaname, tablename, indexname,
                    idx_scan, idx_tup_read, idx_tup_fetch
                FROM pg_stat_user_indexes
                """
            elif database_type == DatabaseType.MYSQL:
                usage_query = """
                SELECT 
                    TABLE_SCHEMA, TABLE_NAME, INDEX_NAME,
                    CARDINALITY
                FROM information_schema.STATISTICS
                WHERE TABLE_SCHEMA = DATABASE()
                """
            else:
                # SQLite doesn't have built-in index usage stats
                return
            
            result = await client.execute_query(usage_query)
            
            # Update usage statistics
            for row in result.data:
                if database_type == DatabaseType.POSTGRESQL:
                    index_name = row.get('indexname')
                    if index_name in self.indexes[database_name]:
                        index_def = self.indexes[database_name][index_name]
                        index_def.scan_count = row.get('idx_scan', 0)
                        index_def.tuple_read = row.get('idx_tup_read', 0)
                        index_def.tuple_fetch = row.get('idx_tup_fetch', 0)
                        index_def.last_used = datetime.now(timezone.utc)
            
        except Exception as e:
            logger.error(f"Failed to monitor index usage for {database_name}: {e}")
    
    async def cleanup_unused_indexes(self, database_name: str, client: AbstractDatabaseClient, 
                                   unused_threshold_days: int = 30) -> List[str]:
        """Remove unused indexes."""
        removed_indexes = []
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=unused_threshold_days)
        
        for index_name, index_def in self.indexes[database_name].items():
            # Skip system indexes and primary keys
            if index_name.startswith('pk_') or index_name.startswith('sys_'):
                continue
            
            # Check if index is unused
            if (index_def.scan_count == 0 and 
                index_def.last_used and 
                index_def.last_used < cutoff_date):
                
                try:
                    # Drop the index
                    drop_sql = f"DROP INDEX {index_name}"
                    await client.execute_query(drop_sql)
                    
                    # Remove from tracking
                    del self.indexes[database_name][index_name]
                    removed_indexes.append(index_name)
                    
                    logger.info(f"ðŸ—‘ï¸ Removed unused index: {index_name}")
                    
                except Exception as e:
                    logger.error(f"âŒ Failed to remove index {index_name}: {e}")
        
        return removed_indexes
    
    async def _get_existing_indexes(self, client: AbstractDatabaseClient) -> Dict[str, IndexDefinition]:
        """Get existing indexes from database."""
        indexes = {}
        
        try:
            database_type = getattr(client.config, 'type', DatabaseType.SQLITE)
            
            if database_type == DatabaseType.POSTGRESQL:
                query = """
                SELECT 
                    indexname, tablename, indexdef
                FROM pg_indexes
                WHERE schemaname = 'public'
                """
            elif database_type == DatabaseType.MYSQL:
                query = """
                SELECT 
                    INDEX_NAME, TABLE_NAME, COLUMN_NAME
                FROM information_schema.STATISTICS
                WHERE TABLE_SCHEMA = DATABASE()
                ORDER BY INDEX_NAME, SEQ_IN_INDEX
                """
            elif database_type == DatabaseType.SQLITE:
                query = """
                SELECT name, tbl_name, sql
                FROM sqlite_master
                WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                """
            else:
                return indexes
            
            result = await client.execute_query(query)
            
            # Parse results into IndexDefinition objects
            for row in result.data:
                if database_type == DatabaseType.SQLITE:
                    index_name = row.get('name')
                    table_name = row.get('tbl_name')
                    # Parse columns from SQL (simplified)
                    columns = ['id']  # Placeholder
                    
                    indexes[index_name] = IndexDefinition(
                        name=index_name,
                        table=table_name,
                        columns=columns,
                        status=IndexStatus.ACTIVE
                    )
            
        except Exception as e:
            logger.error(f"Failed to get existing indexes: {e}")
        
        return indexes
    
    def _filter_existing_indexes(self, recommendations: List[IndexRecommendation], 
                                existing_indexes: Dict[str, IndexDefinition]) -> List[IndexRecommendation]:
        """Filter out recommendations for indexes that already exist."""
        filtered = []
        
        for recommendation in recommendations:
            index_def = recommendation.index_definition
            
            # Check if similar index already exists
            exists = False
            for existing_name, existing_def in existing_indexes.items():
                # Acknowledge unused variable
                _ = existing_name
                if (existing_def.table == index_def.table and
                    set(existing_def.columns) == set(index_def.columns)):
                    exists = True
                    break
            
            if not exists:
                filtered.append(recommendation)
        
        return filtered
    
    def _prioritize_recommendations(self, recommendations: List[IndexRecommendation]) -> List[IndexRecommendation]:
        """Prioritize index recommendations."""
        # Sort by priority, then by estimated improvement
        return sorted(recommendations, 
                     key=lambda x: (x.priority, -x.estimated_improvement))
    
    def get_index_report(self, database_name: str) -> Dict[str, Any]:
        """Generate index usage and recommendation report."""
        indexes = self.indexes.get(database_name, {})
        recommendations = self.recommendations.get(database_name, [])
        
        # Calculate statistics
        total_indexes = len(indexes)
        active_indexes = len([idx for idx in indexes.values() if idx.status == IndexStatus.ACTIVE])
        unused_indexes = len([idx for idx in indexes.values() if idx.scan_count == 0])
        
        return {
            "database": database_name,
            "total_indexes": total_indexes,
            "active_indexes": active_indexes,
            "unused_indexes": unused_indexes,
            "recommendations_count": len(recommendations),
            "high_priority_recommendations": len([r for r in recommendations if r.priority == 1]),
            "estimated_total_improvement": sum(r.estimated_improvement for r in recommendations),
            "indexes": {name: {
                "table": idx.table,
                "columns": idx.columns,
                "type": idx.index_type.value,
                "status": idx.status.value,
                "usage_count": idx.scan_count
            } for name, idx in indexes.items()},
            "top_recommendations": [
                {
                    "index_name": r.index_definition.name,
                    "table": r.index_definition.table,
                    "columns": r.index_definition.columns,
                    "priority": r.priority,
                    "reason": r.reason,
                    "estimated_improvement": r.estimated_improvement
                }
                for r in recommendations[:5]
            ]
        }


# Global instance
index_manager = IndexManager()
