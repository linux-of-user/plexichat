"""
Enhanced self-testing system for PlexiChat with WebUI and GUI integration.
Provides comprehensive testing capabilities with detailed reporting.
"""

import asyncio
import time
import json
import hashlib
import secrets
import psutil
import sqlite3
import gzip
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
import traceback
import subprocess
import sys
import os

import logging import logger


class EnhancedTestResult:
    """Enhanced test result with comprehensive metrics and reporting."""
    
    def __init__(self, name: str, category: str, description: str = ""):
        self.name = name
        self.category = category
        self.description = description
        self.status = "pending"  # pending, running, passed, failed, skipped, warning
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
        self.duration_ms: Optional[float] = None
        self.message = ""
        self.details: Dict[str, Any] = {}
        self.performance_metrics: Dict[str, float] = {}
        self.warnings: List[str] = []
        self.errors: List[str] = []
        self.recommendations: List[str] = []
        self.severity = "info"  # info, warning, error, critical
    
    def start(self):
        """Mark test as started."""
        self.status = "running"
        self.start_time = datetime.now(timezone.utc)
        logger.info(f"üß™ Starting test: {self.name}")
    
    def finish(self, success: bool, message: str = "", details: Dict[str, Any] = None):
        """Mark test as finished."""
        self.end_time = datetime.now(timezone.utc)
        if self.start_time:
            self.duration_ms = (self.end_time - self.start_time).total_seconds() * 1000
        
        if success:
            self.status = "passed" if not self.warnings else "warning"
        else:
            self.status = "failed"
            self.severity = "error"
        
        self.message = message
        if details:
            self.details.update(details)
        
        status_emoji = "‚úÖ" if success else "‚ùå"
        logger.info(f"{status_emoji} Test completed: {self.name} ({self.duration_ms:.1f}ms)")
    
    def add_performance_metric(self, name: str, value: float, unit: str = "", threshold: Optional[float] = None):
        """Add a performance metric with optional threshold checking."""
        self.performance_metrics[name] = {
            "value": value,
            "unit": unit,
            "threshold": threshold,
            "within_threshold": threshold is None or value <= threshold
        }
        
        if threshold and value > threshold:
            self.add_warning(f"{name} ({value}{unit}) exceeds threshold ({threshold}{unit})")
    
    def add_warning(self, warning: str):
        """Add a warning message."""
        self.warnings.append(warning)
        logger.warning(f"‚ö†Ô∏è Test warning in {self.name}: {warning}")
    
    def add_error(self, error: str):
        """Add an error message."""
        self.errors.append(error)
        self.severity = "error"
        logger.error(f"‚ùå Test error in {self.name}: {error}")
    
    def add_recommendation(self, recommendation: str):
        """Add a recommendation for improvement."""
        self.recommendations.append(recommendation)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "name": self.name,
            "category": self.category,
            "description": self.description,
            "status": self.status,
            "severity": self.severity,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration_ms": self.duration_ms,
            "message": self.message,
            "details": self.details,
            "performance_metrics": self.performance_metrics,
            "warnings": self.warnings,
            "errors": self.errors,
            "recommendations": self.recommendations
        }


class EnhancedTestSuite:
    """Enhanced comprehensive testing framework for PlexiChat."""
    
    def __init__(self):
        self.tests: Dict[str, EnhancedTestResult] = {}
        self.test_categories = {
            "system": "System Configuration & Resources",
            "database": "Database Connectivity & Performance",
            "security": "Security & Encryption",
            "backup": "Backup System & Recovery",
            "performance": "Performance & Scalability",
            "network": "Network & Connectivity",
            "integration": "API & Integration",
            "stress": "Stress & Load Testing",
            "government": "Government-Level Security Compliance"
        }
        
        self.test_report_dir = Path("tests/reports")
        self.test_report_dir.mkdir(parents=True, exist_ok=True)
        
        # Performance thresholds
        self.thresholds = {
            "database_query_ms": 100,
            "backup_creation_ms": 30000,
            "encryption_ms": 1000,
            "memory_usage_percent": 80,
            "cpu_usage_percent": 80,
            "disk_usage_percent": 90
        }
    
    async def run_comprehensive_tests(self, categories: Optional[List[str]] = None) -> Dict[str, Any]:
        """Run comprehensive test suite with detailed reporting."""
        logger.info("üöÄ Starting Enhanced PlexiChat Test Suite...")
        
        if categories is None:
            categories = list(self.test_categories.keys())
        
        start_time = datetime.now(timezone.utc)
        
        # Initialize test report
        report = {
            "test_run_id": secrets.token_hex(8),
            "start_time": start_time.isoformat(),
            "categories_tested": categories,
            "system_info": await self._get_system_info(),
            "tests": {},
            "summary": {}
        }
        
        # Run tests by category
        for category in categories:
            logger.info(f"üîç Running {self.test_categories.get(category, category)} tests...")
            await self._run_category_tests(category)
        
        end_time = datetime.now(timezone.utc)
        total_duration = (end_time - start_time).total_seconds()
        
        # Generate comprehensive report
        report.update({
            "end_time": end_time.isoformat(),
            "total_duration_seconds": total_duration,
            "tests": {name: test.to_dict() for name, test in self.tests.items()},
            "summary": self._generate_test_summary()
        })
        
        # Save detailed report
        await self._save_enhanced_report(report)
        
        logger.info(f"‚úÖ Enhanced test suite completed in {total_duration:.2f} seconds")
        return report
    
    async def _get_system_info(self) -> Dict[str, Any]:
        """Get comprehensive system information."""
        try:
            return {
                "platform": sys.platform,
                "python_version": sys.version,
                "cpu_count": psutil.cpu_count(),
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "disk_total_gb": psutil.disk_usage('.').total / (1024**3),
                "hostname": os.uname().nodename if hasattr(os, 'uname') else "unknown",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"Failed to get system info: {e}")
            return {"error": str(e)}
    
    async def _run_category_tests(self, category: str):
        """Run all tests in a specific category."""
        test_methods = {
            "system": self._test_system_configuration,
            "database": self._test_database_functionality,
            "security": self._test_security_features,
            "backup": self._test_backup_system,
            "performance": self._test_performance_metrics,
            "network": self._test_network_connectivity,
            "integration": self._test_api_integration,
            "stress": self._test_stress_scenarios,
            "government": self._test_government_compliance
        }
        
        test_method = test_methods.get(category)
        if test_method:
            await test_method()
        else:
            logger.warning(f"Unknown test category: {category}")
    
    async def _test_system_configuration(self):
        """Test system configuration and resources."""
        # Test 1: Configuration Management
        test = EnhancedTestResult(
            "Configuration Management", 
            "system",
            "Verify all configuration files and directories are properly set up"
        )
        test.start()
        
        try:
            from plexichat.core.config_manager import config_manager
            
            # Initialize configuration
            init_report = config_manager.initialize_application()
            
            test.details["initialization_report"] = init_report
            test.details["created_directories"] = len(init_report.get("created_directories", []))
            test.details["created_configs"] = len(init_report.get("created_configs", []))
            
            # Check for warnings and errors
            warnings = init_report.get("warnings", [])
            errors = init_report.get("errors", [])
            
            for warning in warnings:
                test.add_warning(warning)
            
            for error in errors:
                test.add_error(error)
            
            if init_report.get("success", False):
                test.finish(True, "Configuration management working correctly")
                if not warnings:
                    test.add_recommendation("All configurations are properly set up")
                else:
                    test.add_recommendation("Review configuration warnings and ensure all required files are present")
            else:
                test.finish(False, "Configuration initialization failed")
                test.add_recommendation("Check file permissions and disk space")
            
        except Exception as e:
            test.finish(False, f"Configuration test failed: {e}")
            test.add_error(str(e))
        
        self.tests[test.name] = test
        
        # Test 2: System Resources
        test = EnhancedTestResult(
            "System Resources", 
            "system",
            "Monitor system resource usage and performance"
        )
        test.start()
        
        try:
            # CPU monitoring
            cpu_percent = psutil.cpu_percent(interval=1)
            test.add_performance_metric("cpu_usage_percent", cpu_percent, "%", self.thresholds["cpu_usage_percent"])
            
            # Memory monitoring
            memory = psutil.virtual_memory()
            test.add_performance_metric("memory_usage_percent", memory.percent, "%", self.thresholds["memory_usage_percent"])
            test.add_performance_metric("memory_available_gb", memory.available / (1024**3), "GB")
            
            # Disk monitoring
            disk = psutil.disk_usage('.')
            disk_usage_percent = (disk.used / disk.total) * 100
            test.add_performance_metric("disk_usage_percent", disk_usage_percent, "%", self.thresholds["disk_usage_percent"])
            test.add_performance_metric("disk_free_gb", disk.free / (1024**3), "GB")
            
            # Network interfaces
            network_interfaces = len(psutil.net_if_addrs())
            test.details["network_interfaces"] = network_interfaces
            
            # Process information
            current_process = psutil.Process()
            test.add_performance_metric("process_memory_mb", current_process.memory_info().rss / (1024**2), "MB")
            test.add_performance_metric("process_cpu_percent", current_process.cpu_percent(), "%")
            
            # Generate recommendations
            if cpu_percent > 50:
                test.add_recommendation("Consider monitoring CPU usage during peak loads")
            if memory.percent > 70:
                test.add_recommendation("Monitor memory usage and consider increasing available RAM")
            if disk_usage_percent > 80:
                test.add_recommendation("Consider cleaning up disk space or expanding storage")
            
            test.finish(True, "System resources monitored successfully")
            
        except Exception as e:
            test.finish(False, f"System resource test failed: {e}")
        
        self.tests[test.name] = test
        
        # Test 3: Directory Structure Integrity
        test = EnhancedTestResult(
            "Directory Structure", 
            "system",
            "Verify all required directories exist with proper permissions"
        )
        test.start()
        
        try:
            required_directories = [
                ("config", "Configuration files"),
                ("data", "Application data"),
                ("logs", "Log files"),
                ("uploads", "User uploads"),
                ("secure_backups", "Backup storage"),
                ("temp", "Temporary files")
            ]
            
            directory_status = {}
            permission_issues = []
            
            for dir_name, description in required_directories:
                dir_path = Path(dir_name)
                
                if dir_path.exists():
                    # Check permissions
                    stat_info = dir_path.stat()
                    permissions = oct(stat_info.st_mode)[-3:]
                    
                    directory_status[dir_name] = {
                        "exists": True,
                        "permissions": permissions,
                        "description": description
                    }
                    
                    # Check for security issues
                    if dir_name in ["secure_backups", "keys", "config"] and permissions.endswith('7'):
                        permission_issues.append(f"{dir_name} has world-writable permissions")
                else:
                    directory_status[dir_name] = {
                        "exists": False,
                        "description": description
                    }
                    test.add_warning(f"Directory {dir_name} does not exist")
            
            test.details["directory_status"] = directory_status
            
            for issue in permission_issues:
                test.add_warning(issue)
                test.add_recommendation(f"Secure directory permissions: chmod 700 {issue.split()[0]}")
            
            existing_dirs = sum(1 for status in directory_status.values() if status["exists"])
            test.add_performance_metric("directories_existing", existing_dirs, "count")
            test.add_performance_metric("directories_required", len(required_directories), "count")
            
            if existing_dirs == len(required_directories) and not permission_issues:
                test.finish(True, "All directories exist with proper permissions")
            else:
                test.finish(True, "Directory structure has some issues but is functional")
                test.add_recommendation("Run configuration initialization to create missing directories")
            
        except Exception as e:
            test.finish(False, f"Directory structure test failed: {e}")

        self.tests[test.name] = test

    async def _test_backup_system(self):
        """Test comprehensive backup system functionality."""
        # Test 1: Backup System Initialization
        test = EnhancedTestResult(
            "Backup System Initialization",
            "backup",
            "Test backup system components and configuration"
        )
        test.start()

        try:
            # Test backup directory structure
            backup_dirs = [
                Path("secure_backups"),
                Path("secure_backups/shards"),
                Path("secure_backups/metadata"),
                Path("secure_backups/recovery")
            ]

            missing_dirs = []
            for backup_dir in backup_dirs:
                if not backup_dir.exists():
                    backup_dir.mkdir(parents=True, exist_ok=True)
                    missing_dirs.append(str(backup_dir))

            if missing_dirs:
                test.add_warning(f"Created missing backup directories: {missing_dirs}")

            # Test backup service availability
            try:
                from plexichat.services.enhanced_backup_service import EnhancedBackupService
                test.details["backup_service_available"] = True
            except ImportError:
                test.add_warning("Enhanced backup service not available")
                test.details["backup_service_available"] = False

            test.finish(True, "Backup system initialized successfully")
            test.add_recommendation("Ensure backup service is properly configured for production use")

        except Exception as e:
            test.finish(False, f"Backup initialization failed: {e}")

        self.tests[test.name] = test

        # Test 2: Shard Creation and Management
        test = EnhancedTestResult(
            "Shard Creation Performance",
            "backup",
            "Test shard creation, compression, and verification performance"
        )
        test.start()

        try:
            # Create test data
            test_data_sizes = [1024, 10240, 102400, 1048576]  # 1KB, 10KB, 100KB, 1MB

            for size in test_data_sizes:
                test_data = secrets.token_bytes(size)

                # Test compression
                start_time = time.time()
                compressed = gzip.compress(test_data, compresslevel=9)
                compression_time = (time.time() - start_time) * 1000

                # Test checksum calculation
                start_time = time.time()
                checksum = hashlib.sha256(compressed).hexdigest()
                checksum_time = (time.time() - start_time) * 1000

                # Test decompression
                start_time = time.time()
                decompressed = gzip.decompress(compressed)
                decompression_time = (time.time() - start_time) * 1000

                # Verify integrity
                if decompressed != test_data:
                    test.add_error(f"Data integrity check failed for {size} bytes")

                # Record metrics
                size_label = f"{size//1024}KB" if size >= 1024 else f"{size}B"
                test.add_performance_metric(f"compression_time_{size_label}", compression_time, "ms")
                test.add_performance_metric(f"checksum_time_{size_label}", checksum_time, "ms")
                test.add_performance_metric(f"decompression_time_{size_label}", decompression_time, "ms")
                test.add_performance_metric(f"compression_ratio_{size_label}", len(compressed) / len(test_data))

            test.finish(True, "Shard creation and verification working correctly")
            test.add_recommendation("Monitor compression performance for large datasets")

        except Exception as e:
            test.finish(False, f"Shard creation test failed: {e}")

        self.tests[test.name] = test

        # Test 3: Backup Recovery Simulation
        test = EnhancedTestResult(
            "Backup Recovery Simulation",
            "backup",
            "Simulate backup recovery process with multiple shards"
        )
        test.start()

        try:
            # Simulate creating multiple shards
            original_data = b"PlexiChat Test Data " * 1000  # ~17KB
            shard_size = 5000  # 5KB shards

            # Split into shards
            shards = []
            for i in range(0, len(original_data), shard_size):
                shard_data = original_data[i:i + shard_size]
                compressed_shard = gzip.compress(shard_data)
                checksum = hashlib.sha256(compressed_shard).hexdigest()

                shards.append({
                    "index": len(shards),
                    "data": compressed_shard,
                    "checksum": checksum,
                    "size": len(compressed_shard)
                })

            test.details["total_shards"] = len(shards)
            test.details["original_size"] = len(original_data)
            test.details["compressed_size"] = sum(shard["size"] for shard in shards)

            # Simulate recovery process
            start_time = time.time()

            # Verify all shards
            verified_shards = 0
            for shard in shards:
                calculated_checksum = hashlib.sha256(shard["data"]).hexdigest()
                if calculated_checksum == shard["checksum"]:
                    verified_shards += 1
                else:
                    test.add_error(f"Shard {shard['index']} checksum verification failed")

            # Reconstruct data
            reconstructed_data = b""
            for shard in sorted(shards, key=lambda x: x["index"]):
                decompressed_shard = gzip.decompress(shard["data"])
                reconstructed_data += decompressed_shard

            recovery_time = (time.time() - start_time) * 1000

            # Verify reconstruction
            if reconstructed_data == original_data:
                test.add_performance_metric("recovery_time_ms", recovery_time, "ms")
                test.add_performance_metric("verified_shards", verified_shards, "count")
                test.add_performance_metric("shard_verification_rate", verified_shards / len(shards) * 100, "%")

                test.finish(True, f"Recovery simulation successful: {verified_shards}/{len(shards)} shards verified")
                test.add_recommendation("Implement redundancy to handle missing shards in production")
            else:
                test.finish(False, "Data reconstruction failed")
                test.add_error("Reconstructed data does not match original")

        except Exception as e:
            test.finish(False, f"Recovery simulation failed: {e}")

        self.tests[test.name] = test

    async def _test_security_features(self):
        """Test security and encryption features."""
        # Test 1: Encryption Performance
        test = EnhancedTestResult(
            "Encryption Performance",
            "security",
            "Test encryption/decryption performance and security"
        )
        test.start()

        try:
            from cryptography.fernet import Fernet
            import bcrypt

            # Test symmetric encryption
            key = Fernet.generate_key()
            cipher = Fernet(key)

            test_data_sizes = [1024, 10240, 102400]  # 1KB, 10KB, 100KB

            for size in test_data_sizes:
                test_data = secrets.token_bytes(size)

                # Encryption performance
                start_time = time.time()
                encrypted = cipher.encrypt(test_data)
                encryption_time = (time.time() - start_time) * 1000

                # Decryption performance
                start_time = time.time()
                decrypted = cipher.decrypt(encrypted)
                decryption_time = (time.time() - start_time) * 1000

                # Verify integrity
                if decrypted != test_data:
                    test.add_error(f"Encryption/decryption failed for {size} bytes")

                size_label = f"{size//1024}KB"
                test.add_performance_metric(f"encryption_time_{size_label}", encryption_time, "ms", self.thresholds.get("encryption_ms", 1000))
                test.add_performance_metric(f"decryption_time_{size_label}", decryption_time, "ms")

            # Test password hashing
            password = "test_password_123"
            start_time = time.time()
            hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt(rounds=12))
            hash_time = (time.time() - start_time) * 1000

            start_time = time.time()
            is_valid = bcrypt.checkpw(password.encode(), hashed)
            verify_time = (time.time() - start_time) * 1000

            test.add_performance_metric("password_hash_time_ms", hash_time, "ms")
            test.add_performance_metric("password_verify_time_ms", verify_time, "ms")

            if is_valid:
                test.finish(True, "All encryption tests passed")
                test.add_recommendation("Consider using hardware security modules for production")
            else:
                test.finish(False, "Password verification failed")

        except Exception as e:
            test.finish(False, f"Encryption test failed: {e}")

        self.tests[test.name] = test

        # Test 2: Government Security Compliance
        test = EnhancedTestResult(
            "Government Security Standards",
            "security",
            "Verify compliance with government-level security requirements"
        )
        test.start()

        try:
            compliance_checks = {
                "AES-256 Encryption": False,
                "PBKDF2 Key Derivation": False,
                "Secure Random Generation": False,
                "File Permission Security": False,
                "Memory Protection": False
            }

            # Check AES-256 support
            try:
                from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
                from cryptography.hazmat.primitives import hashes
                from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

                # Test AES-256
                key = secrets.token_bytes(32)  # 256-bit key
                cipher = Cipher(algorithms.AES(key), modes.GCM(secrets.token_bytes(12)))
                compliance_checks["AES-256 Encryption"] = True

                # Test PBKDF2
                kdf = PBKDF2HMAC(
                    algorithm=hashes.SHA256(),
                    length=32,
                    salt=secrets.token_bytes(16),
                    iterations=200000,
                )
                compliance_checks["PBKDF2 Key Derivation"] = True

            except Exception as e:
                test.add_error(f"Cryptography compliance check failed: {e}")

            # Test secure random generation
            try:
                random_data = secrets.token_bytes(32)
                if len(random_data) == 32:
                    compliance_checks["Secure Random Generation"] = True
            except Exception as e:
                test.add_error(f"Secure random generation failed: {e}")

            # Check file permissions
            try:
                import stat
                secure_dirs = ["config", "keys", "secure_backups"]
                secure_permissions = True

                for dir_name in secure_dirs:
                    dir_path = Path(dir_name)
                    if dir_path.exists():
                        mode = dir_path.stat().st_mode
                        if mode & stat.S_IROTH or mode & stat.S_IWOTH:  # World readable/writable
                            secure_permissions = False
                            test.add_warning(f"Directory {dir_name} has insecure permissions")

                compliance_checks["File Permission Security"] = secure_permissions
            except Exception as e:
                test.add_warning(f"File permission check failed: {e}")

            # Memory protection (basic check)
            try:
                # Check if we can clear sensitive data from memory
                sensitive_data = bytearray(b"sensitive" * 100)
                for i in range(len(sensitive_data)):
                    sensitive_data[i] = 0
                compliance_checks["Memory Protection"] = True
            except Exception as e:
                test.add_warning(f"Memory protection check failed: {e}")

            test.details["compliance_checks"] = compliance_checks

            passed_checks = sum(compliance_checks.values())
            total_checks = len(compliance_checks)
            compliance_percentage = (passed_checks / total_checks) * 100

            test.add_performance_metric("compliance_percentage", compliance_percentage, "%")

            if compliance_percentage >= 80:
                test.finish(True, f"Government security compliance: {compliance_percentage:.1f}%")
                if compliance_percentage < 100:
                    test.add_recommendation("Address remaining compliance issues for full government certification")
            else:
                test.finish(False, f"Insufficient security compliance: {compliance_percentage:.1f}%")
                test.add_recommendation("Critical security improvements required for government use")

        except Exception as e:
            test.finish(False, f"Security compliance test failed: {e}")

        self.tests[test.name] = test

    async def _test_database_functionality(self):
        """Test database connectivity and performance."""
        # Test 1: Database Connection
        test = EnhancedTestResult(
            "Database Connection",
            "database",
            "Test database connectivity and basic operations"
        )
        test.start()

        try:
            from plexichat.core.database import get_session

            # Test connection
            start_time = time.time()
            with get_session() as session:
                session.execute("SELECT 1")
            connection_time = (time.time() - start_time) * 1000

            test.add_performance_metric("connection_time_ms", connection_time, "ms", self.thresholds["database_query_ms"])
            test.finish(True, "Database connection successful")
            test.add_recommendation("Monitor database connection pool for production use")

        except ImportError:
            test.add_warning("Database modules not available")
            test.finish(True, "Database modules not imported (expected in some configurations)")
        except Exception as e:
            test.finish(False, f"Database connection failed: {e}")
            test.add_recommendation("Check database configuration and ensure database server is running")

        self.tests[test.name] = test

    async def _test_performance_metrics(self):
        """Test system performance under various conditions."""
        # Test 1: Concurrent Operations
        test = EnhancedTestResult(
            "Concurrent Operations",
            "performance",
            "Test system performance under concurrent load"
        )
        test.start()

        try:
            # Simulate concurrent operations
            async def dummy_operation():
                await asyncio.sleep(0.01)  # 10ms operation
                return hashlib.sha256(secrets.token_bytes(1024)).hexdigest()

            # Test different concurrency levels
            concurrency_levels = [1, 10, 50, 100]

            for level in concurrency_levels:
                start_time = time.time()
                tasks = [dummy_operation() for _ in range(level)]
                results = await asyncio.gather(*tasks)
                total_time = (time.time() - start_time) * 1000

                test.add_performance_metric(f"concurrent_{level}_ops_time_ms", total_time, "ms")
                test.add_performance_metric(f"concurrent_{level}_ops_per_sec", level / (total_time / 1000), "ops/sec")

            test.finish(True, "Concurrent operations test completed")
            test.add_recommendation("Monitor performance under production load patterns")

        except Exception as e:
            test.finish(False, f"Concurrent operations test failed: {e}")

        self.tests[test.name] = test

    async def _test_network_connectivity(self):
        """Test network connectivity and API endpoints."""
        # Test 1: Local Network Interface
        test = EnhancedTestResult(
            "Network Interface",
            "network",
            "Test network interface availability and configuration"
        )
        test.start()

        try:
            import socket

            # Test localhost connectivity
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5)

            # Try to bind to localhost
            try:
                sock.bind(('localhost', 0))
                port = sock.getsockname()[1]
                test.details["test_port"] = port
                sock.close()

                test.finish(True, "Network interface available")
                test.add_recommendation("Ensure firewall settings allow required ports")
            except Exception as e:
                test.finish(False, f"Network binding failed: {e}")

        except Exception as e:
            test.finish(False, f"Network test failed: {e}")

        self.tests[test.name] = test

    async def _test_api_integration(self):
        """Test API integration and endpoints."""
        # Placeholder for API integration tests
        test = EnhancedTestResult(
            "API Integration",
            "integration",
            "Test API endpoints and integration functionality"
        )
        test.start()

        try:
            # This would test actual API endpoints when the server is running
            test.finish(True, "API integration test placeholder")
            test.add_recommendation("Implement comprehensive API endpoint testing")

        except Exception as e:
            test.finish(False, f"API integration test failed: {e}")

        self.tests[test.name] = test

    async def _test_stress_scenarios(self):
        """Test system under stress conditions."""
        # Test 1: Memory Stress Test
        test = EnhancedTestResult(
            "Memory Stress Test",
            "stress",
            "Test system behavior under memory pressure"
        )
        test.start()

        try:
            # Allocate memory in chunks and monitor usage
            memory_chunks = []
            chunk_size = 1024 * 1024  # 1MB chunks
            max_chunks = 100  # Max 100MB

            initial_memory = psutil.virtual_memory().percent

            for i in range(max_chunks):
                chunk = bytearray(chunk_size)
                memory_chunks.append(chunk)

                current_memory = psutil.virtual_memory().percent
                if current_memory > 90:  # Stop if memory usage gets too high
                    break

            final_memory = psutil.virtual_memory().percent
            memory_increase = final_memory - initial_memory

            # Clean up
            del memory_chunks

            test.add_performance_metric("memory_increase_percent", memory_increase, "%")
            test.add_performance_metric("allocated_chunks", i + 1, "count")

            test.finish(True, f"Memory stress test completed: {memory_increase:.1f}% increase")
            test.add_recommendation("Monitor memory usage patterns in production")

        except Exception as e:
            test.finish(False, f"Memory stress test failed: {e}")

        self.tests[test.name] = test

    async def _test_government_compliance(self):
        """Test government-level security and compliance requirements."""
        # Test 1: Data Classification
        test = EnhancedTestResult(
            "Data Classification",
            "government",
            "Test data classification and handling procedures"
        )
        test.start()

        try:
            # Test security levels
            security_levels = ["unclassified", "confidential", "secret", "top_secret"]

            classification_support = {}
            for level in security_levels:
                # Test if the system can handle different classification levels
                test_data = f"Test data classified as {level}".encode()
                encrypted_data = hashlib.sha256(test_data).hexdigest()
                classification_support[level] = len(encrypted_data) == 64  # SHA256 hash length

            test.details["classification_support"] = classification_support

            supported_levels = sum(classification_support.values())
            test.add_performance_metric("supported_classification_levels", supported_levels, "count")

            test.finish(True, f"Data classification support: {supported_levels}/{len(security_levels)} levels")
            test.add_recommendation("Implement proper data labeling and handling procedures")

        except Exception as e:
            test.finish(False, f"Data classification test failed: {e}")

        self.tests[test.name] = test

    def _generate_test_summary(self) -> Dict[str, Any]:
        """Generate comprehensive test summary."""
        total_tests = len(self.tests)
        passed_tests = len([t for t in self.tests.values() if t.status == "passed"])
        failed_tests = len([t for t in self.tests.values() if t.status == "failed"])
        warning_tests = len([t for t in self.tests.values() if t.status == "warning"])

        # Calculate average duration
        durations = [t.duration_ms for t in self.tests.values() if t.duration_ms is not None]
        avg_duration = sum(durations) / len(durations) if durations else 0

        # Count warnings and errors
        total_warnings = sum(len(t.warnings) for t in self.tests.values())
        total_errors = sum(len(t.errors) for t in self.tests.values())

        # Category breakdown
        category_summary = {}
        for category in self.test_categories.keys():
            category_tests = [t for t in self.tests.values() if t.category == category]
            if category_tests:
                category_summary[category] = {
                    "total": len(category_tests),
                    "passed": len([t for t in category_tests if t.status == "passed"]),
                    "failed": len([t for t in category_tests if t.status == "failed"]),
                    "warnings": len([t for t in category_tests if t.status == "warning"])
                }

        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "warning_tests": warning_tests,
            "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            "average_duration_ms": avg_duration,
            "total_warnings": total_warnings,
            "total_errors": total_errors,
            "category_breakdown": category_summary,
            "overall_status": "passed" if failed_tests == 0 else "failed",
            "recommendations_count": sum(len(t.recommendations) for t in self.tests.values())
        }

    async def _save_enhanced_report(self, report: Dict[str, Any]):
        """Save enhanced test report to file."""
        try:
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            report_file = self.test_report_dir / f"enhanced_test_report_{timestamp}.json"

            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2, default=str)

            # Also save a latest report
            latest_file = self.test_report_dir / "latest_test_report.json"
            with open(latest_file, 'w') as f:
                json.dump(report, f, indent=2, default=str)

            logger.info(f"üìä Test report saved: {report_file}")

        except Exception as e:
            logger.error(f"Failed to save test report: {e}")


# Global test suite instance
enhanced_test_suite = EnhancedTestSuite()
